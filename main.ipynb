{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py\n",
    "\n",
    "This is the file where we show all of the graphs and analyses we've performed on our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data and Modules\n",
    "\n",
    "Here, we load all of our data and modules that we're going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "import igraph as ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"data/\"\n",
    "save_frame = False\n",
    "\n",
    "accounts_data = pd.read_csv(PATH_DATA + \"instagram_accounts.csv\", converters={'id_followers': literal_eval, 'department': literal_eval})\n",
    "posts_data1 = pd.read_csv(PATH_DATA + \"instagram_posts.csv\")\n",
    "posts_data2 = pd.read_csv(PATH_DATA + \"instagram_posts_1211_1611.csv\")\n",
    "posts_data = pd.concat([posts_data1, posts_data2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if our duplicate user changes anything ...\n",
    "posts_data = posts_data[posts_data['id_user'] != 603282]\n",
    "posts_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup\n",
    "\n",
    "Here, we check for duplicates, empty values etc. in our Accounts and Posts dataframes. We do end up finding one duplicate user in Accounts so we need to remove him/her, remove him/her from all followers list and recompute the numbers.\n",
    "\n",
    "Posts data, on the other hand, was far nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for empty cells across the data \n",
    "accounts_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates - and we found one!\n",
    "print(accounts_data.shape)\n",
    "duplicated_user_id = accounts_data[ accounts_data['id_user'].duplicated() == True ][\"id_user\"].values\n",
    "print(duplicated_user_id)\n",
    "accounts_data.drop_duplicates(subset=['id_user'], inplace=True, keep=False)\n",
    "accounts_data.reset_index(drop=True, inplace=True)\n",
    "accounts_data.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "print(accounts_data.head())\n",
    "\n",
    "for idx in range(len(accounts_data)):\n",
    "    lst = accounts_data.at[idx, \"id_followers\"]\n",
    "    for ele in duplicated_user_id:\n",
    "        if ele in lst:\n",
    "            lst.remove(ele)\n",
    "    accounts_data.at[idx, \"id_followers\"] = lst\n",
    "\n",
    "accounts_data[\"nb_followers\"] = accounts_data[\"id_followers\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_following = {key: 0 for key in accounts_data[\"id_user\"].values}\n",
    "for idx in range(len(accounts_data)):\n",
    "    lst = accounts_data.at[idx, \"id_followers\"]\n",
    "    for ele in lst:\n",
    "        dict_following[ele] += 1\n",
    "\n",
    "accounts_data.set_index(\"id_user\", inplace = True)\n",
    "for key , value in dict_following.items():\n",
    "    accounts_data.at[key,\"nb_following\"] = value\n",
    "\n",
    "accounts_data.reset_index(inplace=True)\n",
    "accounts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No duplicates across Posts! :)\n",
    "posts_data[ posts_data['id_user'].duplicated() == True ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How our duplicate user fares across posts:\n",
    "# He's a terminal node on the graph, so there's little to no effect from him.\n",
    "posts_data[posts_data[\"id_user\"] == 603282]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_frame == True: \n",
    "    posts_data.to_csv('data/new/instagram_posts.csv')\n",
    "    accounts_data.to_csv('data/new/instagram_accounts.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "In this section, we can take a look at statistical properties of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sex Data\n",
    "accounts_data['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_data['department'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_data[\"nb_followers\"].hist()\n",
    "plt.show()\n",
    "accounts_data[\"nb_followers\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_data[\"nb_following\"].hist()\n",
    "plt.show()\n",
    "accounts_data[\"nb_following\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.to_datetime(accounts_data[\"birth_date\"])\n",
    "age = (pd.to_datetime(\"today\") - age) / np.timedelta64(1, \"Y\")\n",
    "age[age < 0] += 100\n",
    "\n",
    "accounts_data[\"age\"] = age.astype('int32')\n",
    "print(accounts_data[\"age\"].describe())\n",
    "\n",
    "accounts_data[\"age\"].hist(histtype=\"bar\", ec=\"black\")\n",
    "plt.title(\"Ages of our users\")\n",
    "plt.ylabel(\"# of users\")\n",
    "plt.xlabel(\"Binned ages\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## House_Buy Data\n",
    "posts_data['house_buy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data[\"views\"].hist()\n",
    "posts_data[\"views\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data['link_clicks'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data[\"id_post_origin\"].value_counts(sort=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data[[\"id_post\",\"reposts\"]].sort_values(by=\"reposts\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing idiotic timestamps\n",
    "posts_data['time'] = posts_data['time'].apply(lambda str: str.replace(\"00:\", \"12:\"))\n",
    "posts_data['time'] = posts_data['time'].apply(lambda str: str.replace(\"13:\", \"01:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the hourly distribution of posts.\n",
    "posts_data['time24'] = posts_data['time'] + \" \" + posts_data['half_day'].apply(lambda str: str.upper())\n",
    "posts_data['time24'] = pd.to_datetime(posts_data['time24'], format=\"%I:%M %p\")\n",
    "\n",
    "posts_data['time24'].dt.hour.hist(bins=24, rwidth=0.5)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Construction\n",
    "\n",
    "This section constructs the graphs of Accounts and Posts that we'll use in the metrics to follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Accounts_ Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappingFrNodeToUserId = dict(zip(range(len(accounts_data)), accounts_data['id_user']))\n",
    "mappingFrUserIdToNode = {v: k for k,v in mappingFrNodeToUserId.items()}\n",
    "dict_followers = dict( zip(accounts_data['id_user'], accounts_data['id_followers']) )\n",
    "edges=[(mappingFrUserIdToNode[node_i], node_j) for node_i in dict_followers.keys() for node_j in list(map(lambda x: mappingFrUserIdToNode[x], dict_followers[node_i]))]\n",
    "\n",
    "# print(edges)\n",
    "accounts = ig.Graph(edges=edges, directed=True)\n",
    "accounts.vs[\"size\"] = 1\n",
    "accounts.layout_lgl()\n",
    "# igraph.plot(accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_diameters = accounts.get_diameter()\n",
    "print(acc_diameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Conclusion_: Our user have broad connections, not deep ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Posts_ Graph Construction\n",
    "\n",
    "Here we construct the graph for the posts. Each node is identified by its index in the dataframe, and then, its attributes are given as node attributes. All edges are added based on this. \n",
    "- Note: this has to be a directed graph to respect  the flow of information i.e. A -> B means that information flows from A to B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_posts_graph(graph_data): \n",
    "    graph = ig.Graph(directed=True); \n",
    "    graph.add_vertices(graph_data.index.values)\n",
    "    for idx in graph_data.index: \n",
    "        ### if id_post_origin is zero, append it to its own list\n",
    "        #   These are the original posts.\n",
    "        id_origin, id_post, id_user = graph_data.iloc[idx][['id_post_origin', 'id_post','id_user']] \n",
    "        # Add the vertex properties first \n",
    "        graph.vs[idx]['id_post'] = id_post\n",
    "        graph.vs[idx]['id_post_origin'] = id_origin\n",
    "        graph.vs[idx]['id_user'] = id_user    \n",
    "        \n",
    "        # Now, if it's not an origin post, \n",
    "        # we can add the corresponding edge. \n",
    "        if id_origin != 0 :\n",
    "            # For this, we take the id_origin and \n",
    "            # get the index of that post. Then, \n",
    "            # we can connect those two nodes :)\n",
    "            orig_index = graph_data.index[graph_data['id_post'] == id_origin].values[0]\n",
    "            graph.add_edges([ (orig_index, idx) ])\n",
    "\n",
    "    graph.vs['size']=7\n",
    "    graph.vs['arrow_size']=1\n",
    "    graph.vs['arrow_width']=1\n",
    "    return graph\n",
    "posts = construct_posts_graph(posts_data)\n",
    "ig.plot(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the components of our posts-graph\n",
    "post_components = posts.clusters(mode='weak')\n",
    "print(len(post_components))\n",
    "\n",
    "# and getting the users attributed for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Metrics_\n",
    "\n",
    "Let's compute our KPI's!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 1: Interactivity\n",
    "\n",
    "Here, we implement the first metric proposed in the first deliverable. This measures the number of likes, clicks, reposts, donations etc. that each post has. We can use this to create a composite ranking, which then provides us a KPI to maximise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Components\n",
    "\n",
    "We expect 4 components - one associated to each of the 4 original posts used to seed our userbase. And that's what we get ! Of course, we have weak components because we're working in a Directed Acyclic Graph, so we'll need to ignore the direction of our edges to find components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactivity(graph, graph_data):\n",
    "    # We get the nodes representing the original posts \n",
    "    # i.e. have original post ID = 0.\n",
    "    original_posts = graph.vs(id_post_origin_eq=0)\n",
    "    # print('Vertex IDs of the Original Posts: ', original_posts[:]['name'])\n",
    "\n",
    "    components = dict( zip(original_posts[:]['name'], graph.clusters(mode='weak') ) )\n",
    "    # print(\"Length of each component: \", [len(c) for c in components.values()])\n",
    "    cumulative_interactions = {key : {} for key in original_posts[:]['name']} \n",
    "    for key in cumulative_interactions:\n",
    "        cumulative_interactions[key]['like']    = 0\n",
    "        cumulative_interactions[key]['comment'] = 0\n",
    "        cumulative_interactions[key]['repost']  = 0\n",
    "        cumulative_interactions[key]['clicks']  = 0\n",
    "        cumulative_interactions[key]['donations_tag_count'] = 0\n",
    "        cumulative_interactions[key]['donations_value']  = 0\n",
    "\n",
    "    # print(cumulative_interactions)\n",
    "    # print(components)\n",
    "    for comp_idx in components:\n",
    "        for node in components[comp_idx]:\n",
    "            # print(node)\n",
    "            row = graph_data.loc[node]\n",
    "\n",
    "            cumulative_interactions[comp_idx]['like']    += row['likes']#.values[0]\n",
    "            cumulative_interactions[comp_idx]['comment'] += row['comments']#.values[0]\n",
    "            cumulative_interactions[comp_idx]['repost']  += row['reposts']#.values[0]\n",
    "            cumulative_interactions[comp_idx]['clicks']  += row['link_clicks']#.values[0].astype(int)\n",
    "            cumulative_interactions[key]['donations_tag_count'] += row[\"donation_tag\"]\n",
    "            cumulative_interactions[key]['donations_value'] += row['donation_val']\n",
    "\n",
    "    cumulative_interactions = pd.DataFrame(cumulative_interactions)\n",
    "    cumulative_interactions['Total'] = cumulative_interactions.sum(axis=1)\n",
    "\n",
    "    return cumulative_interactions\n",
    "\n",
    "interactivity_df = interactivity(posts, posts_data)\n",
    "print(interactivity_df.head())\n",
    "\n",
    "def get_interactivity_score(interactivity_df):\n",
    "    coeff = {'like': 1, 'comment': 2, 'repost': 3, 'clicks': 4, 'donation_tag':5, \"donations_value\":0}\n",
    "    \n",
    "    return sum(interactivity_df['Total'] * list(coeff.values()))\n",
    "\n",
    "print(\"\\nInteractivity Score: \", get_interactivity_score(interactivity_df))\n",
    "\n",
    "def get_donation_value(interactivity_df):\n",
    "    return interactivity_df.at[\"donations_value\", \"Total\"]\n",
    "\n",
    "print(\"\\nDonation Value: \", get_donation_value(interactivity_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 2 : Reachability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 2a: Visibility\n",
    "Here, we implement a metric proposed as part of the first deliverable, visibility. This measures of the total number of views that the campaign have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visibility(graph, graph_data):\n",
    "    original_posts = graph.vs(id_post_origin_eq=0)\n",
    "    components = dict( zip(original_posts[:]['name'], graph.clusters(mode='weak') ) )\n",
    "    cumulative_visibility = {key : {} for key in original_posts[:]['name']} \n",
    "    for key in cumulative_visibility:\n",
    "        cumulative_visibility[key]['views']   = 0\n",
    "    \n",
    "    for comp_idx in components:\n",
    "        for node in components[comp_idx]:\n",
    "            row = graph_data.loc[node]\n",
    "            cumulative_visibility[comp_idx]['views'] += row['views']#.values[0]\n",
    "\n",
    "    cumulative_visibility = pd.DataFrame(cumulative_visibility)\n",
    "    cumulative_visibility['Total'] = cumulative_visibility.sum(axis=1)\n",
    "    \n",
    "    return cumulative_visibility\n",
    "\n",
    "print(visibility(posts, posts_data))\n",
    "\n",
    "def get_total_visibility(visibility_df):\n",
    "    return visibility_df[\"Total\"].values[0]\n",
    "\n",
    "print(\"Total Visibility: \", get_total_visibility(visibility(posts, posts_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 2b: Virality\n",
    "\n",
    "Virality is the speed at which the campaign was propagated. The notion of speed, will be provided for by the diameter of the each connected components. The diameter is inversely proportional to the speed of the campaign as the diameter denotes how many degree of separation between the source node and the \"furthest\" node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component_diameters(posts): \n",
    "    subgraphs = posts.decompose(mode='weak')\n",
    "    diameters = [subgraph.diameter() for subgraph in subgraphs]\n",
    "    return diameters\n",
    "\n",
    "print(\"Depth of each Original Post:\", get_component_diameters(posts))\n",
    "\n",
    "# diameter_paths = [subgraph.get_diameter() for subgraph in subgraphs]\n",
    "# print(\"Actual Path taken by each Depth-y post: \", diameter_paths)\n",
    "\n",
    "# Conclusion: \n",
    "\n",
    "def get_max_diameter(components_diameters):\n",
    "    return max(components_diameters)\n",
    "\n",
    "print(\"Longest Diameter: \", get_max_diameter(get_component_diameters(posts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Analysis of Previous Campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_properties(posts): \n",
    "    # We get the nodes representing the original posts \n",
    "    # i.e. have original post ID = 0.\n",
    "    original_posts = posts.vs(id_post_origin_eq=0)    \n",
    "    original_posters = original_posts[:]['id_user']\n",
    "    print(original_posters)\n",
    "\n",
    "    # Now we evaluate our criteria\n",
    "    original_posters_nodes = [mappingFrUserIdToNode[x] for x in original_posters]\n",
    "    original_posters_closeness = np.array(accounts.closeness(original_posters_nodes, mode=\"out\"))\n",
    "    original_posters_betweenness = np.array(accounts.betweenness(original_posters_nodes)) / ((accounts.vcount() - 1) * (accounts.vcount() - 2))\n",
    "    original_posters_pagerank = np.array(accounts.pagerank(original_posters_nodes))\n",
    "    \n",
    "    # and we return them properly formatted \n",
    "    return pd.DataFrame.from_dict({\n",
    "         'nodes': original_posters_nodes, \n",
    "         'closeness': original_posters_closeness, \n",
    "         'betweenness': original_posters_betweenness, \n",
    "         'pagerank': original_posters_pagerank, \n",
    "        })\n",
    "print(\"Node properties of our most important posts: \")\n",
    "print(node_properties(posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Network's best K nodes\n",
    "K = 4\n",
    "\n",
    "def get_k_nodes_random (nodes = K):\n",
    "    accounts_random = range(len(accounts_data))\n",
    "    return np.random.choice(accounts_random, K, replace=False)\n",
    "\n",
    "print(get_k_nodes_random(nodes=K))\n",
    "\n",
    "def get_k_nodes_closeness(nodes = K):\n",
    "    accounts_closeness = np.array(accounts.closeness(mode=\"out\"))\n",
    "    accounts_closeness_bestKnodes = np.argpartition(accounts_closeness,-K)[-K:]\n",
    "    return np.flip(accounts_closeness_bestKnodes)\n",
    "\n",
    "print(get_k_nodes_closeness(nodes = K))\n",
    "\n",
    "def get_k_nodes_betweenness(nodes = K):\n",
    "    accounts_betweenness = np.array(accounts.betweenness()) / ((accounts.vcount() - 1) * (accounts.vcount() - 2))\n",
    "    accounts_betweenness_bestKnodes = np.argpartition(accounts_betweenness,-K)[-K:]\n",
    "    return np.flip(accounts_betweenness_bestKnodes)\n",
    "\n",
    "print(get_k_nodes_betweenness(nodes = K))\n",
    "\n",
    "def get_k_nodes_pagerank(nodes= K):\n",
    "    accounts_pagerank = np.array(accounts.pagerank())\n",
    "    accounts_pagerank_bestKnodes = np.argpartition(accounts_pagerank,-K)[-K:]\n",
    "    return np.flip(accounts_pagerank_bestKnodes)\n",
    "\n",
    "print(get_k_nodes_pagerank(nodes= K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of Clicks\n",
    "\n",
    "The probability that a user who has seen a post will click on the link to the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_click = posts_data['link_clicks'].sum()/len(posts_data)\n",
    "print(\"Probability of clicking on a post: \", prob_click*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of Donation\n",
    "\n",
    "Evaluated as the number of donors over the number of possible donors(number of site visitors). `prob_donation` gives the probability that someone donated given that they clicked on the link to the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors= posts_data[posts_data[\"donation_tag\"]]\n",
    "print( \"Number of users who went to the website: \", len(posts_data[posts_data[\"link_clicks\"]]) )\n",
    "print (\"Number of donors:\", len(donors[donors['donation_val']>0]))\n",
    "prob_donation =  len(donors[donors['donation_val']>0])/len(posts_data[posts_data[\"link_clicks\"]])\n",
    "print(\"Empirical probabilty of donation: \", prob_donation*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "\n",
    "Here, we finally begin our simulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged Dataset\n",
    "\n",
    "The `merged_dataset` is a tool for us to quickly access users and posts without excessive querying - nothing to worry about :)\n",
    "\n",
    "The model starts right after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset_all = posts_data.merge(accounts_data, on=[\"id_user\"], how='left')\n",
    "merged_dataset = pd.DataFrame(merged_dataset_all[[\"id_user\", \"views\", \"reposts\", \"likes\", \"comments\", \"link_clicks\", \"donation_tag\", \"donation_val\", \"nb_followers\"]])\n",
    "merged_dataset[\"percent_views\"] = merged_dataset[\"views\"]/merged_dataset[\"nb_followers\"]\n",
    "merged_dataset[\"percent_reposts\"] = merged_dataset[\"reposts\"]/merged_dataset[\"views\"]\n",
    "merged_dataset[\"percent_likes\"] = merged_dataset[\"likes\"]/merged_dataset[\"views\"]\n",
    "merged_dataset[\"percent_comments\"] = merged_dataset[\"comments\"]/merged_dataset[\"views\"]\n",
    "# merged_dataset[merged_dataset[\"percent_comments\"]> 1]\n",
    "\n",
    "merged_dataset.set_index(['id_user'],inplace=True)\n",
    "merged_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Regression Model for Donation Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector \n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel_data = merged_dataset_all[merged_dataset_all[\"donation_val\"] > 0]\n",
    "\n",
    "# X_variables_columns = [\"nb_followers\", \"nb_following\", \"time24\", \"nb_posts\", \"sex\", \"department\", \"age\"]\n",
    "X_variables_columns = [\"nb_followers\", \"nb_following\", \"time24\", \"nb_posts\", \"department\", \"age\"]\n",
    "X_variables = pd.DataFrame(mlmodel_data[X_variables_columns])\n",
    "X_variables[\"time24\"] = X_variables[\"time24\"].apply(lambda x : x.hour)\n",
    "X_variables[\"department\"] = X_variables[\"department\"].apply(lambda x : x[1])\n",
    "\n",
    "target_column = [\"donation_val\"]\n",
    "target = mlmodel_data[target_column]\n",
    "target = target.to_numpy().reshape(len(target),)\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X_variables)\n",
    "categorical_columns = categorical_columns_selector(X_variables)\n",
    "\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = MinMaxScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "    ('minmax_scaler', numerical_preprocessor, numerical_columns)])\n",
    "\n",
    "regr = SGDRegressor(max_iter=10000)\n",
    "mlmodel = make_pipeline(preprocessor, regr)\n",
    "\n",
    "_ = mlmodel.fit(X_variables, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_donation_value(id_user):\n",
    "    X_variables_columns = [\"nb_followers\", \"nb_following\", \"time24\", \"nb_posts\", \"department\", \"age\"]\n",
    "    X_var = merged_dataset_all[merged_dataset_all[\"id_user\"] == id_user]\n",
    "    X_var = pd.DataFrame(X_var[X_variables_columns])\n",
    "    X_var[\"time24\"] = X_var[\"time24\"].apply(lambda x : x.hour)\n",
    "    X_var[\"department\"] = X_var[\"department\"].apply(lambda x : x[1])    \n",
    "    \n",
    "    donation_val = mlmodel.predict(X_var)\n",
    "\n",
    "    return max(0, int(donation_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Compartment Model under consideration.\n",
    "\n",
    "Poster ---> Followers ----> \n",
    "\n",
    "                            View ----> Reposts\n",
    "\n",
    "                            View ----> Comment\n",
    "\n",
    "                            View ----> Like\n",
    "\n",
    "Poster ---> Link Click ---> Donation\n",
    "\n",
    "The simulation essentially works as follows. \n",
    "\n",
    "We achieve this using a simple Breadth First Traversal of the accounts graph, assuming that the best possible seeds have been chosen to start from. This choice of seeds effectively decides the strategy. \n",
    "\n",
    "In case of a repost, we populate a simulated `posts_data` data called `new_posts_data`. \n",
    "\n",
    "The number of reposts is given by the outdegree of the graph. This is because each outgoing edge in the `Posts` graph represents a poster->reposter link between posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = lambda n, p : np.random.uniform(0, 1, n) < p\n",
    "\n",
    "## strategy: {\"random\", \"closeness\", \"betweenness\", \"pagerank\"}\n",
    "def simulation(strategy, nodes):\n",
    "    strategies = {\"random\": get_k_nodes_random, \"closeness\": get_k_nodes_closeness, \"betweenness\": get_k_nodes_betweenness, \"pagerank\": get_k_nodes_pagerank}\n",
    "    # Select the initial seeds x\n",
    "    seeded_nodes = strategies[strategy](nodes=K)\n",
    "    # seeded_nodes = [0, 1, 2, 3] # seeds from previous campaign\n",
    "    new_posts_data = pd.DataFrame(columns=['id_user', 'id_post', 'views',\n",
    "        'reposts', 'likes', 'comments', 'id_post_origin', 'link_clicks',\n",
    "        'donation_tag', 'donation_val'])\n",
    "\n",
    "    # Seeding the table\n",
    "    counter=1\n",
    "    for node in seeded_nodes: \n",
    "        id_user = accounts_data.at[node,'id_user']\n",
    "        row = merged_dataset.loc[id_user]\n",
    "        data = accounts_data[accounts_data['id_user'] == node]\n",
    "\n",
    "\n",
    "        # Creating the entries \n",
    "        followers = np.array( data['id_followers'] ) \n",
    "        new_views = followers[rand(followers.size, row['percent_views'])]\n",
    "\n",
    "        # And then filter the number of likes and comments it gets ....\n",
    "        new_likes = new_views[rand(new_views.size, row['percent_likes'])]\n",
    "        new_comments = new_views[rand(new_views.size, row['percent_comments'])]\n",
    "        \n",
    "        # Finally the number of clicks and donors\n",
    "        new_click = np.random.random() < prob_click\n",
    "        new_donor = False\n",
    "        donation_value = 0\n",
    "        if new_click: \n",
    "            # Probability of donating\n",
    "            new_donor = np.random.random() < prob_donation\n",
    "        if new_donor:\n",
    "            donation_value = get_predicted_donation_value(id_user)\n",
    "            if donation_value <= 0:\n",
    "                new_donor = False\n",
    "\n",
    "        # Entering the entries \n",
    "        new_posts_data = new_posts_data.append({\n",
    "            'id_user': id_user, \n",
    "            'id_post': counter,\n",
    "            'id_post_origin': 0, \n",
    "            'views': len(new_views),\n",
    "            'likes': len(new_likes), \n",
    "            'comments': len(new_comments),\n",
    "            'reposts':  0, # A posteriori \n",
    "            'link_clicks': new_click, \n",
    "            'donation_tag': new_donor, \n",
    "            'donation_val': donation_value, \n",
    "        }, ignore_index=True)\n",
    "        counter += 1\n",
    "        \n",
    "    # Initialisation of our long walk \n",
    "    frontier  = [accounts_data.at[node,'id_user'] for node in seeded_nodes] ## To begin with seeded nodes\n",
    "    seen_list = { key : False for key in accounts_data['id_user'] }\n",
    "    for node in frontier: \n",
    "        seen_list[node] = True\n",
    "    \n",
    "    # The long walk ...\n",
    "    while len(frontier) > 0: \n",
    "        node = frontier.pop(0)\n",
    "        data = accounts_data[accounts_data['id_user'] == node]\n",
    "        row  = merged_dataset.loc[node] \n",
    "        followers = np.array( data['id_followers'].values[0] ) \n",
    "        # Get the number of viewers\n",
    "        new_views = followers[rand(followers.size, row['percent_views'])]\n",
    "\n",
    "        # And then filter the number of likes and comments it gets ....\n",
    "        new_likes = new_views[rand(new_views.size, row['percent_likes'])]\n",
    "        new_comments = new_views[rand(new_views.size, row['percent_comments'])]\n",
    "\n",
    "        # Finally the number of clicks and donors\n",
    "        new_click = np.random.random() < prob_click\n",
    "        new_donor = False\n",
    "        donation_value = 0\n",
    "        if new_click: \n",
    "            # Probability of donating\n",
    "            new_donor = np.random.random() < prob_donation\n",
    "        if new_donor:\n",
    "            donation_value = get_predicted_donation_value(id_user)\n",
    "            if donation_value <= 0:\n",
    "                new_donor = False\n",
    "            \n",
    "        # And the only ones who'll get entries into the table - the reposters.\n",
    "        percent_reposts = row['percent_reposts'] if row['percent_reposts'] < 1 else 1\n",
    "        new_reposts = followers[rand(followers.size, percent_reposts)] #\n",
    "        new_reposts = np.array( [follower for follower in new_views if seen_list[follower] == False] )\n",
    "        # print(new_views)\n",
    "\n",
    "        # Only reposts get into the new_posts hall of fame ... \n",
    "        for nbor in new_reposts: \n",
    "            if seen_list[nbor] == False: \n",
    "                id_user = nbor #accounts_data[accounts_data['id_user'] == node]\n",
    "                row = merged_dataset.loc[id_user]\n",
    "                # print(new_posts_data)\n",
    "                new_posts_data = new_posts_data.append({\n",
    "                    'id_user': id_user, \n",
    "                    'id_post': counter,\n",
    "                    'id_post_origin': new_posts_data.loc[new_posts_data['id_user']==node]['id_post'].values[0], \n",
    "                    'views': len(new_views),\n",
    "                    'likes': len(new_likes),\n",
    "                    'comments': len(new_comments),\n",
    "                    'reposts':  0,                 \n",
    "                    'link_clicks': new_click, \n",
    "                    'donation_tag': new_donor, \n",
    "                    'donation_val': donation_value,              \n",
    "                }, ignore_index=True)\n",
    "                counter += 1 \n",
    "                seen_list[nbor] = True \n",
    "                # if repost_list[nbor] == False:\n",
    "                #     repost_list[nbor] = True\n",
    "                frontier.append(nbor)\n",
    "    \n",
    "    new_posts_data.reset_index(drop=True, inplace=True)\n",
    "    new_posts_data.set_index('id_user') \n",
    "\n",
    "    new_posts = construct_posts_graph(new_posts_data)\n",
    "    new_posts_data['reposts'] = pd.Series( new_posts.degree(mode='out') )\n",
    "\n",
    "    return new_posts_data, new_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI Evaluation: Simulation Edition\n",
    "\n",
    "Let's see how well we did, as compared to the original campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_posts_data, new_posts = simulation(strategy=\"closeness\", nodes=K)\n",
    "ig.plot(new_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_posts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visibility(new_posts, new_posts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_components = new_posts.clusters(mode='weak')\n",
    "print(\"Number of components in the NewPosts Graph:\", len(new_components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactivity(new_posts, new_posts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The diameters of each component are: \", get_component_diameters(new_posts) )\n",
    "print('Plotting the smallest subgraph for visual understanding ... ')\n",
    "\n",
    "subgr = new_posts.decompose(mode='weak')\n",
    "ig.plot(subgr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_SIMULATIONS = 50\n",
    "strategy = \"pagerank\"\n",
    "\n",
    "new_posts_data_and_graph_monte_carlo = []\n",
    "\n",
    "for i in range (NUM_OF_SIMULATIONS):\n",
    "    simu_posts_data, simu_posts = simulation(strategy=strategy, nodes=K)\n",
    "    new_posts_data_and_graph_monte_carlo.append((simu_posts_data,simu_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visibility_monte_carlo = list(map(lambda x: visibility(x[1],x[0]) ,new_posts_data_and_graph_monte_carlo))\n",
    "# print(visibility_monte_carlo)\n",
    "average_visibility = sum(visibility_monte_carlo)/len(visibility_monte_carlo)\n",
    "print(average_visibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactivity_monte_carlo = list(map(lambda x: interactivity(x[1],x[0]) ,new_posts_data_and_graph_monte_carlo))\n",
    "# print(interactivity_monte_carlo)\n",
    "average_interactivity = sum(interactivity_monte_carlo)/len(interactivity_monte_carlo)\n",
    "print(average_interactivity)\n",
    "\n",
    "average_interactivity_score = get_interactivity_score(average_interactivity)\n",
    "print(\"\\nAverage Interactivity Score: \", average_interactivity_score)\n",
    "\n",
    "average_donation_value = get_donation_value(average_interactivity)\n",
    "print(\"\\nAverage Donation Value: \", average_donation_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virality_monte_carlo = list(map(lambda x: get_component_diameters(x[1]), new_posts_data_and_graph_monte_carlo))\n",
    "average_virality = np.array(virality_monte_carlo).mean(axis=0)\n",
    "print(average_virality)\n",
    "\n",
    "print(\"Longest Diameter: \", get_max_diameter(average_virality))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Simulation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_runs = list(range(1,NUM_OF_SIMULATIONS + 1))\n",
    "visibility_data = list(map(get_total_visibility, visibility_monte_carlo))\n",
    "interactivity_score_data = list(map(get_interactivity_score, interactivity_monte_carlo))\n",
    "donation_value_data = list(map(get_donation_value, interactivity_monte_carlo))\n",
    "virality_data = list(map(get_max_diameter, virality_monte_carlo))\n",
    "\n",
    "simulation_df_inputs_dict = {\"runs\": simulation_runs, \"visibility\": visibility_data, \"interactivity score\": interactivity_score_data, \"donation_value\": donation_value_data, \"diameter\": virality_data}\n",
    "simulation_df = pd.DataFrame(simulation_df_inputs_dict)\n",
    "simulation_df.set_index([\"runs\"], inplace=True)\n",
    "simulation_df.to_csv(\"./simulation_data/{}{}.csv\".format(strategy, NUM_OF_SIMULATIONS))\n",
    "simulation_df.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7688b97a3db1c1b0876cf0d2d8381c4f33d17628277d8270a04fbe35bb4e111"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
